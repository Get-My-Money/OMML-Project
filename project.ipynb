{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "__Federated Learning__\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc8f17470001d71b"
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-10T15:21:21.570308700Z",
     "start_time": "2023-11-10T15:21:21.559204300Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import sklearn.metrics\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Data Preprocessing__"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9cbc988c8786bc33"
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"mushrooms.csv\")\n",
    "df = pd.get_dummies(df, drop_first=True).astype(float)\n",
    "y = df.class_p\n",
    "X = df.drop('class_p', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=239)\n",
    "X_train = torch.from_numpy(X_train.to_numpy()).to(device, dtype=torch.float32)\n",
    "X_test = torch.from_numpy(X_test.to_numpy()).to(device, dtype=torch.float32)\n",
    "y_train = torch.from_numpy(y_train.to_numpy()).to(device, dtype=torch.float32)\n",
    "y_test = torch.from_numpy(y_test.to_numpy()).to(device, dtype=torch.float32)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T15:46:02.701886200Z",
     "start_time": "2023-11-10T15:46:01.478863100Z"
    }
   },
   "id": "e9c92b13cabae697"
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "outputs": [],
   "source": [
    "class GradientPair:\n",
    "    \n",
    "    def __init__(self, dw: torch.Tensor, db: torch.Tensor):\n",
    "        self.dw: torch.Tensor = dw\n",
    "        self.db: torch.Tensor = db\n",
    "        \n",
    "\n",
    "class LogisticRegressionModel:\n",
    "    def __init__(self, train_sample):\n",
    "        self.weights: torch.Tensor = torch.from_numpy(\n",
    "            np.zeros(train_sample.size(dim=0))\n",
    "        ).to(dtype=torch.float64).to(device)\n",
    "        self.bias = torch.Tensor([0]).to(dtype=torch.float64).to(device)\n",
    "        \n",
    "    @staticmethod\n",
    "    def fromWeights(train_sample, w, b):\n",
    "        model = LogisticRegressionModel(train_sample)\n",
    "        model.weights = w\n",
    "        model.bias = b\n",
    "        return model\n",
    "        \n",
    "    def calculateProbability(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # print(self.weights.dtype)\n",
    "        \n",
    "        return 1 / (\n",
    "                1 + torch.exp(\n",
    "                    -1 * self.weights.T @ x - self.bias\n",
    "                )\n",
    "        )\n",
    "    \n",
    "    def calculateLossAndGradient(self, x: torch.Tensor, y: torch.Tensor) -> tuple[torch.Tensor, GradientPair]:\n",
    "        param_num = x.size(dim=0)\n",
    "        prediction = self.calculateProbability(x)\n",
    "        loss = (\n",
    "                       -1 / param_num) * (y * torch.log(prediction)\n",
    "        ) + (\n",
    "                (torch.Tensor([1]).to(device) - y) * torch.log(torch.Tensor([1]).to(device) - prediction)\n",
    "        )\n",
    "        \n",
    "        wGrad = (1 / param_num) * (\n",
    "            (prediction - y) * x\n",
    "        )\n",
    "        bGrad = (1 / param_num) * (\n",
    "            (prediction - y)\n",
    "        )\n",
    "        return loss, GradientPair(wGrad, bGrad)\n",
    "    \n",
    "    def updateWeights(self, delta_w: torch.Tensor, delta_b: torch.Tensor) -> None:\n",
    "        self.weights += delta_w\n",
    "        self.bias += delta_b\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return 0 if self.calculateProbability(X) <= 0.5 else 1\n",
    "        \n",
    "    def test(self, X_test, y_test):\n",
    "        results = []\n",
    "        for i in range(X_test.size(dim = 0)):\n",
    "            arguments = X_test[0]\n",
    "            results.append(self.predict(arguments))\n",
    "        return accuracy_score(y_test.to(\"cpu\"), results)\n",
    "            "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T15:47:39.568317600Z",
     "start_time": "2023-11-10T15:47:39.543427Z"
    }
   },
   "id": "a8726dbaa607ad54"
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "outputs": [],
   "source": [
    "class FederatedLearningNode:\n",
    "\n",
    "    def __init__(self, alpha, p, n, arguments, labels):\n",
    "        \"\"\"\n",
    "        \n",
    "        :param alpha: learning rate for mode \n",
    "        :param p: probability of choosing node training\n",
    "        :param n: number of nodes participating in training\n",
    "        :param arguments: Train data arguments\n",
    "        :param labels: Train data labels\n",
    "        \"\"\"\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.p = p\n",
    "        self.n = n\n",
    "        self.arguments = arguments\n",
    "        self.labels = labels\n",
    "        \n",
    "        self.local_model = LogisticRegressionModel(self.arguments[0])\n",
    "        self.index = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "    def localTrainStep(self):\n",
    "        \"\"\"\n",
    "        THIS METHOD MUST BE OVERWRITTEN FOR SPECIFIC ALGORITHM\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "\n",
    "class FederatedLearningAlgorithm:\n",
    "    def __init__(self, arguments: pd.DataFrame = None, labels: pd.Series = None, alpha: float = 0.1, lam: float = 0.01, p: float = 0.5, k: int = 1000, n: int = 3):\n",
    "        \"\"\"\n",
    "            :param alpha: float in [0; 1] Learning rate of gradient descent\n",
    "            :param lam: float in [0; +inf); difference parameter for psi-quadratic penalty. If lam -> 0 => several models are local only, is lam -> +inf, all local models are the same\n",
    "            :param p: float in [0; 1]; probability of choosing the local step instead og aggregation. p -> 0 => only local model training without aggregation; p -> 1 => only aggregation without training (useless)\n",
    "            :param k: integer number of learning iterations\n",
    "            :param n: integer amount of nodes participating in the learning\n",
    "            :param arguments: pandas.Dataframe with train data arguments\n",
    "            :param labels: pandas.Series with train data labels\n",
    "        \"\"\"\n",
    "        assert arguments.shape[0] == labels.size\n",
    "        self.arguments = torch.from_numpy(arguments.to_numpy()).to(device)\n",
    "        self.labels = torch.from_numpy(labels.to_numpy()).to(device)\n",
    "        self.alpha = alpha\n",
    "        self.lam = lam\n",
    "        self.p = p\n",
    "        self.k = k\n",
    "        self.n = n\n",
    "        self.nodes = []\n",
    "        print(type(self.arguments))\n",
    "        self.commonModel = LogisticRegressionModel(self.arguments[0])\n",
    "        \n",
    "        self.constructNodes()\n",
    "        \n",
    "    \n",
    "    def constructNodes(self) -> None:\n",
    "        \"\"\"\n",
    "        THIS METHOD MUST BE OVERWRITTEN IN EACH ALGORITHM CLASS TO CONSTRUCT THE LIST OF NODES WITH APPROPRIEATE INSTANCES\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def train(self) -> None:\n",
    "        for i in range(self.k):\n",
    "            localTraining = random.random() < self.p\n",
    "            if localTraining:\n",
    "                for node in self.nodes:\n",
    "                    node.localTrainStep()\n",
    "            else:\n",
    "                self.aggregateResults()\n",
    "            \n",
    "    def aggregateResults(self) -> None:\n",
    "        \"\"\"\n",
    "        THIS METHOD MUST BE OVERWRITTEN FOR SPECIFIC ALGORITHM\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    \n",
    "        \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T15:47:40.594208700Z",
     "start_time": "2023-11-10T15:47:40.589121300Z"
    }
   },
   "id": "ab0b6665101250"
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "outputs": [],
   "source": [
    "class L2GDNode (FederatedLearningNode):\n",
    "    \n",
    "    def localTrainStep(self):\n",
    "        gradient_result = self.local_model.calculateLossAndGradient(\n",
    "            self.arguments[self.index],\n",
    "            self.labels[self.index]\n",
    "        )[1]\n",
    "        nabla_f_w = gradient_result.dw\n",
    "        nabla_f_b = gradient_result.db\n",
    "        delta_w = -1 * self.alpha / (self.n * (1 - self.p)) * nabla_f_w\n",
    "        delta_b = -1 * self.alpha / (self.n * (1 - self.p)) * nabla_f_b\n",
    "        self.local_model.updateWeights(delta_w, delta_b)\n",
    "\n",
    "\n",
    "class L2GD(FederatedLearningAlgorithm):\n",
    "    \n",
    "    def constructNodes(self) -> None:\n",
    "        self.nodes = [\n",
    "            L2GDNode(\n",
    "                self.alpha,\n",
    "                self.p,\n",
    "                self.n,\n",
    "                self.arguments[self.labels.size(dim=0) // self.n * i : self.labels.size(dim=0) // self.n * (i + 1)],\n",
    "                self.labels[self.labels.size(dim=0) // self.n * i : self.labels.size(dim=0) // self.n * (i + 1)]\n",
    "            )\n",
    "            for i in range(self.n)\n",
    "        ]\n",
    "        \n",
    "    def aggregateResults(self) -> None:\n",
    "        w_mean = torch.zeros(self.arguments[0].size(dim = 0)).to(device)\n",
    "        b_mean = torch.zeros(1).to(device)\n",
    "        for node in self.nodes:\n",
    "            w_mean += 1 / self.n * node.local_model.weights\n",
    "            b_mean += 1 / self.n * node.local_model.bias\n",
    "        for node in self.nodes:\n",
    "            al_np = self.alpha * self.lam / self.n / self.p\n",
    "            node.local_model.weights *= (1 - al_np)\n",
    "            node.local_model.bias *= (1 - al_np)\n",
    "            node.local_model.updateWeights(\n",
    "                al_np * w_mean,\n",
    "                al_np * b_mean\n",
    "            )\n",
    "            \n",
    "        test_model = LogisticRegressionModel.fromWeights(self.arguments[0], w_mean, b_mean)\n",
    "        print(\n",
    "            test_model.test(\n",
    "                X_test, y_test\n",
    "            )\n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T15:47:41.222325700Z",
     "start_time": "2023-11-10T15:47:41.201751100Z"
    }
   },
   "id": "1ed08b78d3779ea9"
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "l2gd = L2GD(\n",
    "    arguments=X,\n",
    "    labels=y\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T15:47:41.829983700Z",
     "start_time": "2023-11-10T15:47:41.804678800Z"
    }
   },
   "id": "3c29f3eaa7d93008"
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5128683327116748\n",
      "0.5128683327116748\n",
      "0.5128683327116748\n",
      "0.5128683327116748\n",
      "0.5128683327116748\n",
      "0.5128683327116748\n",
      "0.5128683327116748\n",
      "0.5128683327116748\n",
      "0.5128683327116748\n",
      "0.5128683327116748\n",
      "0.5128683327116748\n",
      "0.5128683327116748\n",
      "0.5128683327116748\n",
      "0.5128683327116748\n",
      "0.5128683327116748\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[192], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m l2gd\u001B[38;5;241m.\u001B[39mtrain()\n",
      "Cell \u001B[1;32mIn[189], line 73\u001B[0m, in \u001B[0;36mFederatedLearningAlgorithm.train\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     71\u001B[0m         node\u001B[38;5;241m.\u001B[39mlocalTrainStep()\n\u001B[0;32m     72\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 73\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maggregateResults()\n",
      "Cell \u001B[1;32mIn[190], line 46\u001B[0m, in \u001B[0;36mL2GD.aggregateResults\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     39\u001B[0m     node\u001B[38;5;241m.\u001B[39mlocal_model\u001B[38;5;241m.\u001B[39mupdateWeights(\n\u001B[0;32m     40\u001B[0m         al_np \u001B[38;5;241m*\u001B[39m w_mean,\n\u001B[0;32m     41\u001B[0m         al_np \u001B[38;5;241m*\u001B[39m b_mean\n\u001B[0;32m     42\u001B[0m     )\n\u001B[0;32m     44\u001B[0m test_model \u001B[38;5;241m=\u001B[39m LogisticRegressionModel\u001B[38;5;241m.\u001B[39mfromWeights(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39marguments[\u001B[38;5;241m0\u001B[39m], w_mean, b_mean)\n\u001B[0;32m     45\u001B[0m \u001B[38;5;28mprint\u001B[39m(\n\u001B[1;32m---> 46\u001B[0m     test_model\u001B[38;5;241m.\u001B[39mtest(\n\u001B[0;32m     47\u001B[0m         X_test, y_test\n\u001B[0;32m     48\u001B[0m     )\n\u001B[0;32m     49\u001B[0m )\n",
      "Cell \u001B[1;32mIn[188], line 59\u001B[0m, in \u001B[0;36mLogisticRegressionModel.test\u001B[1;34m(self, X_test, y_test)\u001B[0m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(X_test\u001B[38;5;241m.\u001B[39msize(dim \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m)):\n\u001B[0;32m     58\u001B[0m     arguments \u001B[38;5;241m=\u001B[39m X_test[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m---> 59\u001B[0m     results\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredict(arguments))\n\u001B[0;32m     60\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m accuracy_score(y_test\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m), results)\n",
      "Cell \u001B[1;32mIn[188], line 53\u001B[0m, in \u001B[0;36mLogisticRegressionModel.predict\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict\u001B[39m(\u001B[38;5;28mself\u001B[39m, X):\n\u001B[1;32m---> 53\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m0\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcalculateProbability(X) \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.5\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m1\u001B[39m\n",
      "Cell \u001B[1;32mIn[188], line 25\u001B[0m, in \u001B[0;36mLogisticRegressionModel.calculateProbability\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcalculateProbability\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: torch\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[0;32m     23\u001B[0m     \u001B[38;5;66;03m# print(self.weights.dtype)\u001B[39;00m\n\u001B[1;32m---> 25\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m/\u001B[39m (\n\u001B[0;32m     26\u001B[0m             \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m torch\u001B[38;5;241m.\u001B[39mexp(\n\u001B[0;32m     27\u001B[0m                 \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweights\u001B[38;5;241m.\u001B[39mT \u001B[38;5;241m@\u001B[39m x \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias\n\u001B[0;32m     28\u001B[0m             )\n\u001B[0;32m     29\u001B[0m     )\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:40\u001B[0m, in \u001B[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     38\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m has_torch_function(args):\n\u001B[0;32m     39\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(wrapped, args, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m---> 40\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m     42\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mNotImplemented\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:913\u001B[0m, in \u001B[0;36mTensor.__rdiv__\u001B[1;34m(self, other)\u001B[0m\n\u001B[0;32m    911\u001B[0m \u001B[38;5;129m@_handle_torch_function_and_wrap_type_error_to_not_implemented\u001B[39m\n\u001B[0;32m    912\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__rdiv__\u001B[39m(\u001B[38;5;28mself\u001B[39m, other):\n\u001B[1;32m--> 913\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreciprocal() \u001B[38;5;241m*\u001B[39m other\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "l2gd.train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T15:47:50.832992500Z",
     "start_time": "2023-11-10T15:47:42.455653600Z"
    }
   },
   "id": "5456082bdbb06c9a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
