{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "__Federated Learning__\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc8f17470001d71b"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-11T18:04:55.691656400Z",
     "start_time": "2023-11-11T18:04:55.684110700Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import sklearn.metrics\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# from models import LogisticRegressionModel\n",
    "# from Tensor import Tensor\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Data Preprocessing__"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9cbc988c8786bc33"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "device = torch.device(f'cuda:{torch.cuda.current_device()}') if torch.cuda.is_available() else 'cpu'\n",
    "torch.set_default_device(device)\n",
    "torch.set_default_dtype(torch.float32)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T18:06:15.501310700Z",
     "start_time": "2023-11-11T18:06:15.495886600Z"
    }
   },
   "id": "e587067321cfb4a0"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"mushrooms.csv\")\n",
    "df = pd.get_dummies(df, drop_first=True).astype(float)\n",
    "y = df.class_p\n",
    "X = df.drop('class_p', axis=1)\n",
    "X_train, X_test, y_train, y_test = (\n",
    "    Tensor(i.to_numpy()) for i in train_test_split(X, y, test_size=0.33, random_state=239)\n",
    ")\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T18:06:16.843601400Z",
     "start_time": "2023-11-11T18:06:16.791250500Z"
    }
   },
   "id": "e9c92b13cabae697"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T18:04:55.740709100Z",
     "start_time": "2023-11-11T18:04:55.740709100Z"
    }
   },
   "id": "1f5af6a8db427c41"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[46], line 82\u001B[0m\n\u001B[0;32m     79\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mupdateParameters(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39malpha \u001B[38;5;241m*\u001B[39m dL_dW, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39malpha \u001B[38;5;241m*\u001B[39m dL_db)\n\u001B[0;32m     81\u001B[0m GD \u001B[38;5;241m=\u001B[39m NaiveGradientDescent(\u001B[38;5;241m0.01\u001B[39m, \u001B[38;5;241m1000\u001B[39m, \u001B[38;5;241m10\u001B[39m)\n\u001B[1;32m---> 82\u001B[0m GD\u001B[38;5;241m.\u001B[39mtrain(X_train, y_train)\n",
      "Cell \u001B[1;32mIn[46], line 77\u001B[0m, in \u001B[0;36mNaiveGradientDescent.train\u001B[1;34m(self, X_train, y_train)\u001B[0m\n\u001B[0;32m     75\u001B[0m     y_pred[i] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mpredict(x)\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m     76\u001B[0m     y_true[i] \u001B[38;5;241m=\u001B[39m y_train[i]\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m---> 77\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mcalculateLoss(y_pred, y_true))\n\u001B[0;32m     78\u001B[0m dL_dW, dL_db \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mcalculateGradients(batch, y_pred, y_true)\n\u001B[0;32m     79\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mupdateParameters(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39malpha \u001B[38;5;241m*\u001B[39m dL_dW, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39malpha \u001B[38;5;241m*\u001B[39m dL_db)\n",
      "Cell \u001B[1;32mIn[46], line 32\u001B[0m, in \u001B[0;36mLogisticRegressionModel.calculateLoss\u001B[1;34m(self, y_pred, y_true)\u001B[0m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcalculateLoss\u001B[39m(\u001B[38;5;28mself\u001B[39m, y_pred: Tensor, y_true: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m     30\u001B[0m     y_zero_loss \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdot(y_true, torch\u001B[38;5;241m.\u001B[39mlog(y_pred \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1e-9\u001B[39m))\n\u001B[0;32m     31\u001B[0m     y_one_loss \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdot(\n\u001B[1;32m---> 32\u001B[0m         Tensor(y_true\u001B[38;5;241m.\u001B[39msize()[\u001B[38;5;241m0\u001B[39m]) \u001B[38;5;241m-\u001B[39m y_true,\n\u001B[0;32m     33\u001B[0m         torch\u001B[38;5;241m.\u001B[39mlog(Tensor(np\u001B[38;5;241m.\u001B[39mones(y_pred\u001B[38;5;241m.\u001B[39msize()[\u001B[38;5;241m0\u001B[39m])) \u001B[38;5;241m-\u001B[39m y_pred)\n\u001B[0;32m     34\u001B[0m     )\n\u001B[0;32m     35\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m*\u001B[39m (y_zero_loss \u001B[38;5;241m+\u001B[39m y_one_loss) \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparams_amount\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\_device.py:77\u001B[0m, in \u001B[0;36mDeviceContext.__torch_function__\u001B[1;34m(self, func, types, args, kwargs)\u001B[0m\n\u001B[0;32m     75\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m func \u001B[38;5;129;01min\u001B[39;00m _device_constructors() \u001B[38;5;129;01mand\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdevice\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     76\u001B[0m     kwargs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdevice\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice\n\u001B[1;32m---> 77\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "class LogisticRegressionModel:\n",
    "\n",
    "    \"\"\"\n",
    "    For outer optimizer working cycle should be:\n",
    "    For k in 1.....N:\n",
    "        y_pred = [model.predict(x_i) for random x_i in X]\n",
    "        loss = model.calculateLoss(y_pred, y_true)\n",
    "        dL_dW, dL_db = model.calculateGradients(X, y_pred=y_pred, y_true=y_true)\n",
    "\n",
    "        ### Compute the update for the weights\n",
    "\n",
    "        model.scaleParameters(scale_factor)  (optionally)\n",
    "        model.updateParameters(delta_W, delta_B)\n",
    "    \"\"\"\n",
    "    def __init__(self, params_amount: int):\n",
    "        self.weights = Tensor(np.zeros(params_amount))\n",
    "        self.bias = Tensor([0])\n",
    "        self.params_amount = params_amount\n",
    "\n",
    "    def calculateZ(self, X: Tensor) -> Tensor:\n",
    "        return self.weights @ X.T + self.bias\n",
    "\n",
    "    def predict(self, x: Tensor) -> Tensor:\n",
    "        z = self.calculateZ(x)\n",
    "        if z.item() >= 0:\n",
    "            return Tensor(1 / (1 + torch.exp(-1 * z)))\n",
    "        return torch.exp(z) / (1 + torch.exp(z))\n",
    "\n",
    "    def calculateLoss(self, y_pred: Tensor, y_true: Tensor) -> Tensor:\n",
    "        y_zero_loss = torch.dot(y_true, torch.log(y_pred + 1e-9))\n",
    "        y_one_loss = torch.dot(\n",
    "            Tensor(y_true.size()[0]) - y_true,\n",
    "            torch.log(Tensor(np.ones(y_pred.size()[0])) - y_pred)\n",
    "        )\n",
    "        return -1 * (y_zero_loss + y_one_loss) / self.params_amount\n",
    "\n",
    "    @staticmethod\n",
    "    def calculateGradients(X: Tensor, y_pred: Tensor, y_true: Tensor) -> tuple[Tensor, Tensor]:\n",
    "        difference = y_pred - y_true\n",
    "        dL_db = Tensor(torch.mean(difference))\n",
    "        JW = X.T @ difference\n",
    "        dL_dW = Tensor([torch.mean(grad) for grad in JW])   # VERY UNCERTAIN ABOUT THIS IN MATHEMATICAL SENSE\n",
    "        return dL_dW, dL_db\n",
    "\n",
    "    def updateParameters(self, deltaW: Tensor, deltaB: Tensor) -> None:\n",
    "        self.weights = self.weights + deltaW\n",
    "        self.bias = self.bias + deltaB\n",
    "\n",
    "    def scaleParameters(self, scale_factor: float) -> None:\n",
    "        self.weights = Tensor(scale_factor * self.weights)\n",
    "        self.bias = Tensor(scale_factor * self.bias)\n",
    "\n",
    "    def predictClass(self, X: Tensor) -> int:\n",
    "        return 1 if self.predict(X).item() >= 0.5 else 0\n",
    "    \n",
    "\n",
    "\n",
    "class NaiveGradientDescent:\n",
    "    \n",
    "    def __init__(self, alpha: float, iterations: int, batch_size: int):\n",
    "        self.alpha = alpha\n",
    "        self.iterations = iterations\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def train(self, X_train, y_train):\n",
    "        self.model = LogisticRegressionModel(X_train.size()[1])\n",
    "        for k in range(self.iterations):\n",
    "            y_pred = torch.zeros(self.batch_size)\n",
    "            y_true= torch.zeros(self.batch_size)\n",
    "            batch = torch.zeros((self.batch_size, X_train.size()[1]))\n",
    "            for i in range(self.batch_size):\n",
    "                random_index = random.randint(0, y_train.size()[0] - 1)\n",
    "                x = X_train[random_index]\n",
    "                batch[i] = x\n",
    "                y_pred[i] = self.model.predict(x).item()\n",
    "                y_true[i] = y_train[i].item()\n",
    "            print(self.model.calculateLoss(y_pred, y_true))\n",
    "            dL_dW, dL_db = self.model.calculateGradients(batch, y_pred, y_true)\n",
    "            self.model.updateParameters(-1 * self.alpha * dL_dW, -1 * self.alpha * dL_db)\n",
    "\n",
    "GD = NaiveGradientDescent(0.01, 1000, 10)\n",
    "GD.train(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T18:06:18.339226300Z",
     "start_time": "2023-11-11T18:06:18.278911500Z"
    }
   },
   "id": "ab0b6665101250"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-11T18:04:55.794277200Z"
    }
   },
   "id": "be96bde3a284bdf7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
